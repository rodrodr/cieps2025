---
title: "Preprocesamiento de textos"
subtitle: "Fundamentos y primeros pasos"
bibliography: references.bib
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE, warning = F, message = F)

biblio <- bibtex::read.bib("references.bib")

# Cargar los paquetes empleados en el analisis
library(readtext)
library(quanteda)
library(quanteda.textstats)
library(reactable)
library(rvest)
library(readr)

# Carga el paquete tenet con herramientas para
# el analisis de texto 
library(tenet)

```



## Introducción

Una vez tengamos los archivos descargados y guardados en la nube o en una carpeta en el disco duro de nuestro ordenador, podemos ir un paso más allá para abrirlos en R, extraer información y limpiarlos si resulta necesario. En esta sección, veremos cómo abrir archivos con distintos formatos (txt, PDF, docx entre otros) y extraer texto de PDFs sin tratar y limpiar los archivos antes de pasar a la siguiente fase de organización de los corpora.

Este pequeño apartado tiene tres secciones. La primera **abre archivos** de texto en R que serán luego empleados para la creación de un todo coherente, relativamente comparable, que se someterá al análisis (corpus). La segunda realiza un **OCR** en archivos en formato PDF para, luego, extraer el texto. Finalmente, la tercera utiliza un conjunto de funciones de para la limpieza y búsqueda sistemática de texto, así como introduce las  **expresiones regulares**. 


## Abrir archivos de textos

El primer paso de cualquier análisis de texto consiste en abrir los textos en el R para su posterior procesamiento y análisis. Afortunadamente, existe una serie de opciones que facilitan mucho la apertura de una cantidad grande de textos de un solo golpe, sin la necesidad de ir de uno en uno.

La función *readtext()* del paquete homónimo lee desde archivos de textos a PDFs, documentos de Word y otros formatos como planillas de Excel o json. No solo lee un archivo de cada vez, sino todavía mejor. Basta con suministrar el camino hasta la carpeta y la función trata de importar todos los archivos ahí contenidos de un golpe. 

En la sección sobre *web scraping* hemos bajado el texto completo de más de 800 libros en español disponibles en los servidores del Proyecto Gutenberg. Los hemos guardado todos en la carpeta "Gut_txt/Archivos/". Ahora podemos abrirlos de una vez en R utilizando la función *readtext()*. Veamos cómo se hace: 

```{r eval=T, warning=F}

# Abre el paquete readtext
library(readtext)
library(reactable)

li <- c("https://github.com/rodrodr/tenet_texts/raw/refs/heads/main/Ficcion/Cervantes-Quijote.txt",
        "https://github.com/rodrodr/tenet_texts/raw/refs/heads/main/Ficcion/Anonimo-Lazarillo_de_Tormes.txt",
        "https://github.com/rodrodr/tenet_texts/raw/refs/heads/main/Ficcion/Pirandello-Seis_Personajes.txt",
        "https://github.com/rodrodr/tenet_texts/raw/refs/heads/main/Ficcion/Sarmiento-Argentina.txt",
        "https://github.com/rodrodr/tenet_texts/raw/refs/heads/main/Ficcion/Sarmiento-Facundo.txt")

# Abre todos los archivos
gt <- readtext(li)

# Visualiza
reactable(gt, wrap=F, resizable = T)

```

<br>

El mismo procedimiento se puede llevar a cabo con archivos PDF que ya han sido sometidos a un OCR o desde un primer momento son digitales. Como vemos abajo, el código es exactamente el mismo. Lo único que cambia es la dirección de la carpeta, que en esta ocasión contiene solamente archivos PDF:

```{r eval=F, warning=F}

# Carga los paquetes
library(readtext)
library(reactable)

# Extrae los textos de todos los archivos en la carpeta
gt <- readtext("../Carpeta/PDFs/")

# Visualiza los 10 primeros
reactable(gt[1:10,], wrap=F, resizable = T)

```

<br>

Como en el caso anterior, el R genera un *data.frame* con dos variables: *doc_id*, conteniendo el nombre del archivo, y *text* con el texto completo. Este nuevo objeto será utilizado luego para la creación de objeto de tipo *corpus* en la tercera parte de esta sección.

## OCR y extracción de texto

Sin embargo, el mundo sería un lugar más aburrido si las cosas siempre fueran tan sencillas. En muchos casos, nos encontraremos con archivos PDF escaneados con una resolución baja y sin reconocimiento de caracteres. En estos casos, nos vemos forzados a procesar los archivos antes de poder llevar a cabo cualquier análisis.

En R, el paquete *tesseract* permite realizar el reconocimiento óptico de caracteres (OCR, en su acrónimo original en inglés) en múltiples archivos y en distintas lenguas. Combinado con el paquete *pdftools*, permiten extraer el texto desde fuentes difíciles de tratar.

Utilizaremos los mismos PDFs para realizar el OCR y luego extraer los textos. El análisis se dividirá en dos partes. En la primera, generaremos una lista de los archivos a ser procesados y descargaremos el modelo de OCR para español.

```{r eval=F, warning=F}

# Carga los paquetes
library(tesseract)
library(pdftools)

# Genera la lista de todos los PDFs
fl <- list.files("../Carpeta/PDFs/")

# Baja el modelo para realizar el 
# OCR en espaniol (solo una vez)
tesseract_download("spa")

# Establece el espaniol como 
# lengua para el OCR
esp <- tesseract("spa")

```

En la segunda parte, utilizaremos un bucle *for* para ir de archivo en archivo, realizar el OCR, extraer el texto y guardarlo en un nuevo formato (.txt) en una nueva carpeta.

```{r eval=F, warning=F}

# Para cada PDF
for (i in 1:length(fl)){
  
  # Informa el avace
  print(paste0(i, " of ", length(fl)))
  
  # Extrae el nombre del archivo
  ls <- unlist(strsplit(fl[i],"/"))
  ls <- gsub(".pdf","", ls)
  ls <- ls[length(ls)]
  
  # Realiza el OCR
  text <- tesseract::ocr(
    paste0("../Carpeta/PDFs/",fl[i]), 
    engine = esp)
  
  # Guarda el resultado en formato texto 
  write(text, paste0("../Carpeta/PDFs/OCR_txt/",ls,".txt"))
  
}

```

Ahora podemos averiguar los resultados obtenidos por medio de la función *readtext()* utilizando el mismo código que hemos visto antes:

```{r eval=F, warning=F}

# Carga los paquetes
library(readtext)
library(reactable)

# Extrae los textos de todos los archivos en la carpeta
gt <- readtext("../Carpeta/PDFs/OCR_txt/")

# Visualiza los 10 primeros
reactable(gt[1:10,], wrap=F, resizable = T)

```


## Manipulación y limpieza de textos

La limpieza de los datos resulta fundamental para obtener un análisis adecuado de los textos. Se trata de un proceso laborioso, pero muy importante para la obtención de datos comparables. Aúna un conjunto de tareas concretas de manipulación que incluye: remover espacios en blanco, tildes, saltos de línea innecesarios o la extracción de datos o metadatos.

Lo que veremos aquí es un conjunto de técnicas que se pueden adaptar a textos de distinta estructura y naturaleza. No existe una solución universal de tratamiento de datos que funcione igual para *tweets* o para textos legales. En el caso de los primeros, habrá que tratar los elementos no textuales o los "emojis" antes de analizar el contenido. En los segundos, suele haber mucho ruido por la repetición de los encabezados de página por su publicación en archivos PDF.

Además, cada estructura nos brindará oportunidades distintas de extracción y análisis de los datos. Por ejemplo, textos legales suelen ser muy estructurados y contienen la identificación de actores, títulos, capítulos, etc. Podemos utilizar tales informaciones como "marcadores" o "etiquetas" a la hora de extraer datos de forma sistemática. Por esa razón, resulta muy útil empezar por la sencilla tarea de explorar y describir cuál es la estructura del texto. ¿Se trata de un texto uniforme o segmentado (divisiones de capítulos, partes, títulos, artículos o cualquier otra)? ¿El texto tiene un formato digital desde el principio o tenemos que tratar encabezados u otros elementos comunes en PDFs y documentos Word? ¿El texto abre con todas las letras legibles o aparecen símblos raros en las tildes? O sea, ¿está en la codificación de caracteres adecuada o tengo que abrirlo utilizando una codificación específica ("LATIN1" es la más común para los que trabajamos textos en español)? La función *stri_enc_list()* del paquete *stringi* proporciona un listado completo de las codificaciones.

En esta parte del laboratorio, veremos algunas técnicas de manipulación de textos que permiten prepararlos para el análisis. Dividiremos el contenido en tres secciones. La primera examina las funciones de manipulación de texto de R y de los paquetes *stringr* y *stringi*. La segunda introduce brevemente las expresiones regulares, que representan un recurso muy útil para la identificación de patrones en textos. Finalmente, la tercera aplica el contenido de las dos anteriores en los textos que empleamos de ejemplo: los libros en español del Proyecto Gutenmberg y los decretos presidenciales de Paraguay.

### Cuenta, busca, extrae, divide, combina, sustituye, compara

Existe un número amplio de funciones en R para la manipulación de texto. Podemos hacer casi cualquier operación desde buscar expresiones concretas hasta combinar textos o transformarlos en otras estructuras. Aquí exploraremos algunas tareas básicas muy útiles para trabajar con textos en R.

#### Cuenta

Una tarea de análisis de texto consiste en contar las veces que determinados temas, contenidos o conceptos aparecen. Esto se puede hacer utilizando ciertas palabras o diccionarios que ayudan a definir el peso de un tópico en el conjunto de elementos de un texto.

Por ejemplo, ¿cuántas veces aparecen palabras que empiezan con "demo" en una variable? La función *stri_count()* del paquete *stringi* retorna el número de texto que un patrón cualquiera (en nuestro caso "demo") aparece en un texto o en una variable.

```{r, eval=T, class.output="vectout"}

# Crea una variable de texto
tx <- "La democracia es la forma de gobierno originada a partir del demos, o pueblo."

# Carga el paquete stringr
library(stringi)

# Cuenta las palabras que contienen "demo"
stri_count(tx, regex = "demo")

# Ahora con una variable
tx <- c("democracia","demostenes democrático","nada","demora")

# Cuenta las palabras que contienen "demo"
# para cada elemento
stri_count(tx, regex = "demo")

```

Como vemos, en el primer caso, el R nos ha retornado las dos veces en las que alguna palabra conteniendo "demo" aparecía en la frase. En el segundo, dos elementos llaman la atención. Primero, ya no es el total de veces en general, sino que el número se divide por observación de la variable. Segundo, debemos tener cuidado con la raíz que utilizamos para evitar ambiguedades y generar falsos positivos. Por ejemplo, demostenes y demora no tienen ninguna relación con democracia.   

Otra forma de contar que puede ser útil en algunos procesos de manipulación de texto. Por ejemplo, los códigos INE de los municipios de España incluyen dos caracteres iniciales con el código de la provincia y luego tres caracteres con el orden alfabético del municipio. Así que Almería tiene el código "04" y está en el 13º puesto en orden alfabético. No obstante, muchas veces, ciertas agencias informan el código como "04013" mientras otras lo informan como "4013". Sin tratamiento, esto resulta un problema a la hora de comparar los datos.  

La función *stri_length()* del paquete *stringi* soluciona el problema al contar cuántos caracteres hay en cada observación de una variable de texto. A partir de ese dato, podemos identificar cuáles elementos debemos tratar. En el ejemplo abajo añadimos un 0 al texto solo para aquellos códigos que son menores de 5 caracteres. De ese modo, uniformizamos el sistema de acuerdo con el estándar definido por el INE:

```{r, eval=T, class.output="vectout"}

# Crea una variable con los códigos INE para los municipios de
# Almería, Barcelona, Madrid, Salamanca y Zamora
tx <- c("4013","8019","28079","37274","49275")

# Carga el paquete
library(stringi)

# Cuenta los caracteres
stri_length(tx)

# Incluye un cero en el codigo del municipio
tx[stri_length(tx)<5] <- paste0("0", tx[stri_length(tx)<5]) 

# Inspecciona los resultados
tx

```

#### Busca

En otras ocasiones, lo que deseamos es saber cuáles elementos del texto contienen ciertas ideas o palabras-clave que buscamos. En ese caso, se trata de identificar o si dichas expresiones se encuentran o no en el texto o, al reves, aquellos textos que contienen la palabra.

Por ejemplo, ¿cuáles elementos de una variable contienen la palabra ministerio o ministro? La función *stri_detect()* del paquete *stringi* lleva a cabo dicha tarea.

```{r, eval=T, class.output="vectout"}

# Crea una variable con distinto contenido
tx <- c("ministro de telecomunicaciones",
        "secretaria adjunto de la presidencia", 
        "ministerio de agricultura", 
        "director de la policía nacional",
        "ministerio de seguridad social",
        "ministra de educación",
        "secretaría nacional de derechos humanos",
        "directoría de asuntos exteriores")

# Carga el paquete sgtringi
library(stringi)

# Detecta cuáles elementos contienen ministro o ministerio
stri_detect(tx, regex = "minist")

# Podemos seleccionarlos si queremos
tx[stri_detect(tx, regex = "minist")]

```

Como se puede observar, el R retorna los elementos de la variable que contienen el patrón "minist". En la primera forma es solamente una indicando TRUE o FALSE. En la segunda, hemos pedido que nos regrese el texto completo de cada observación.

**Ejercicio:** Podéis ejercitar el nuevo conocimiento intentando buscar "secretario" o "secretaría" y "director" o "diretoría".


#### Extrae

En otras ocasiones, queremos extraer los patrones para, por ejemplo, contar el número de veces que ocurren. En el siguiente ejemplo, extraeremos del discurso de investidura de Pedro Sánchez todas las palabras empezadas por igual (igualdad, igualitario, etc.) y por libert (libertad, libertades). Esto es posible gracias a la función *stri_extract_all()* del paquete *stringi*.

```{r, eval=T, class.output="vectout"}

# Carga el paquete readtext
library(readtext)

# Lee el discurso de investidura de Pedro Sánchez de 2020
tx <- readtext("https://github.com/rodrodr/tenet_texts/raw/refs/heads/main/spa.inaugural/15_XIV_Leg_Sanchez.txt")

# Carga el paquete stringi
library(stringi)

# Extrae las palabras con raiz igual
fr <- unlist(stri_extract_all(tx, regex = "igual[a-z]+"))

# Cuenta la frecuencia
table(fr)

# Extrae las palabras con raiz libert
fr <- unlist(stri_extract_all(tx, regex = "libert[a-z]+"))

# Cuenta la frecuencia
table(fr)

```

Se ve como hay una frecuencia mayor de palabras relacionadas a la igualdad que a la libertad, aunque estas últimas también estén presentes en una proporción no muy inferior.

#### Divide

Otra tarea de manipulación de textos consiste en dividirlos según diferentes criterios que requieren cada análisis. Por ejemplo, una tarea muy común consiste en fragmentar los textos en palabras, algo que se denomina *tokenization*. Hay dos funciones en el paquete *stringi* que nos permiten dividir un texto: *stri_split()*, que utiliza un patrón para dividirlo y *stri_split_fixed()* que limita el número de fragmentos. Veamos un ejemplo: 

```{r, eval=T, class.output="vectout"}

# Crea una variable de texto
tx <- "Pepi, Luci, Bom y otras chicas del montón."

# Carga el paquete stringr
library(stringi)

# Separa utilizando la coma
stri_split(tx, regex =",")

# Separa utilizando la coma, pero en solo dos fragmentos
stri_split_fixed(str = tx, pattern = ", ", n = 2)

# Un poco más avanzado - separa utilizando tanto la coma
# como la y
stri_split(tx, regex ="[,y]")

```


#### Combina

En algunas ocasiones, necesitamos combinar distintos textos para trabajar con términos compuestos, bigramas o cualquier otra finalidad. El código abajo nos enseña cómo hacerlo utilizando la función *stri_join()* del paquete *stringi*.

```{r, eval=T, class.output="vectout"}

# Crea una variable de cantidades
val <- c(1, 2, 3, 4)

# Crea una variable de texto
tx <- c("coche", "bicicletas", "hijos", "libros")

# Carga el paquete stringr
library(stringi)

# Combina los dos textos 
stri_join(val, tx, sep=" ", collapse=", ")


```


#### Sustituye

La sustitución resulta muy útil para trabajar textos cuando se require el reemplazo de un valor por otro. Imaginemos que eres profesor y tienes una clase de 130 estudiantes. Como eres atento, les enviarás un informe con las notas por correo electrónico. Ya tienes un archivo Excel con sus nombres y calificaciones. No obstante, resulta muy trabajoso escribir a cada uno copiando y pegando el mismo texto. 

La función *stri_replace*, del paquete que ya conocéis, permite reemplazar los datos como nombre y nota y facilitar el trabajo de redacción. Luego, se pueden utilizar otros paquetes como el *gmailr* para enviar los correos de forma automatizada (este último paso no lo haremos aquí).

```{r, eval=T, tidy.opts=list(width.cutoff = 60), class.output="vectout"}

# Crea una variable de texto
tx <- "EstimadART NOMBRE,\n\nEspero que este correo le encuentre bien.\n\nComo prometido, envío la calificación de la asignatura.\nSu nota final ha sido NOTA.EMOJI\n\nReciba un cordial saludo,\n\nRodrigo\n\n"

# Pongamos unos emojis solo para divertirnos.
emo <- c("\U1F937","\U1F64C","\U1F44D","\U1F947")

# Crea una lista de nombres
nm <- c("Pepe", "Manuel","María","Lola")

# Lista de notas
nota <- c(0, 5, 7.5, 8)

# Artículo definido
art <- c("o","o","a","a")

# Carga el paquete stringr
library(stringi)

# Reemplaza el nombre
st <- stri_replace(tx, nm, regex ="NOMBRE")

# Reemplaza el artículo definido
st <- stri_replace(st, art, regex ="ART")

# Reemplaza el emoji
st <- stri_replace(st, emo, regex ="EMOJI")

# Ahora reemplaza la nota
st <- stri_replace(st, nota, regex ="NOTA")

# Imprime los resultados
cat(st)

```


#### Compara

Otra tarea muy útil consiste en comparar textos y determinar su similitud. Imaginemos que comparamos direcciones, o nombres de personas o entidades en fuentes que pueden contener errores ortográficos o de digitación. En esos casos, resulta fundamental poder medir el grado de similitud o diferencia para tomar una decisión sobre si se trata de la misma entidad o no.

El paquete *stringdist* posee diversas funciones orientadas a esta finalidad. Que permiten comparar desde dos textos entre sí hasta múltiple textos entre ellos.

Ilustremos cómo hacerlo utilizando dos textos literarios. En junio de 1580 muere en Lisboa el poeta Luis de Camões. En septiembre de este mismo año, Madrid asiste a la llegada al mundo de otro inmenso escritor, Francisco de Quevedo. Cualquier nativo o hablante fluyente de portugués o español no puede dejar de sorprenderse por la similitud entre dos sonetos de ambos autores sobre el amor. Incluso, en algunas estrofas, la redacción es idéntica.

El objetivo del código abajo resulta comparar ambos sonetos, estrofa por estrofa, y determinar el grado de similitud entre ellas. Nos restringiremos aquí solamente a algoritmos de similitud que comparan palabras sin atenernos a su función sintáctica o la carga semántica que conlleva. Por lo tanto, se trata de un análisis sencillo de la estructura de las estrofas. 

```{r, eval=T}

# Soneto del amor (Luis de Camões, 1598 - póstumo)
cam1598 <- c("amor es fuego que arde sin verse",
            "es herida que duele y no se siente",
            "es un contentamiento descontento",
            "es dolor que lastima sin doler",
            "es un no querer mas que bien querer",
            "es andar solitario entre la gente",
            "es nunca contentarse de contento",
            "es un cuidar que gana en perderse",
            "es querer estar aprisionado por voluntad",
            "es servir a quien vence, el vencedor",
            "es tener con quien nos mata, lealtad",
            "pero cómo causar puede su favor",
            "en los corazones humanos amistad",
            "si tan contrario a si mismo es el amor")
  
# Soneto del amor (Francisco de Quevedo, 1670 - póstumo)  
qev1670 <- c("es yelo abrasador, es fuego helado", 
            "es herida que duele y no se siente",
            "es un soñado bien, un mal presente",
            "es un breve descanso muy cansado",
            "es un descuido que nos da cuidado",
            "un cobarde, con nombre de valiente",
            "un andar solitario entre la gente",
            "un amar solamente ser amado",
            "es una libertad encarcelada",
            "que dura hasta el postrero parasismo",
            "enfermedad que crece si es curada",
            "este es el nino amor, este es su abismo",
            "mirad cual amistad tendra con nada",
            "el que en todo es contrario de si mismo")  


# Carga los paquetes stringdist - para calcular la similitud 
# y reshape2 - para cambiar el formato de un data.frame
library(stringdist)
library(reshape2)

# Calcula la matriz de similitud entre los dos textos
# La matriz permitirá identificar estrofas incluso si
# se ha cambiado el orden.
rd <- round(stringsimmatrix(cam1598, 
                            qev1670, 
                            method = "lcs"),2)

# Establece los nombres del las lineas como de Camões
# y el nombre de las columnas como de Quevedo
rownames(rd) <- cam1598
colnames(rd) <- qev1670

# Transforma la matriz en un data.frame
drd <- melt(rd)

# Da nombre a las variables
names(drd) <- c("Camoes","Quevedo","Similitud")

# Selecciona solamente los resultados cuya 
# similitud resulta superior a 50%.
drd <- drd[drd$Similitud>0.5,]

# Ordena las estrofas restantes de la más
# similar a la menos
drd <- drd[order(drd$Similitud, decreasing = T),]

# Inspecciona los resultados
reactable(data = drd, resizable = T, striped = T)

```

<br>

Se puede ver que, al menos cuatro estrofas de las 14 (28,6%) son muy similares. Resulta claro que esos dos textos presentan un fuerte parentesco e indican que Quevedo ha sido lector de Camões. Este mismo método puede ser aplicado para cualquier otro tipo de texto. El aspecto crucial es la elección de la unidad de comparación básica. En este ejemplo, la estrofa se empleó como unidad de análisis. En otras fuentes quizás párrafos o cuasi-frases sean las más indicadas. Siempre hay que explorar diferentes posibilidades y métodos antes de aplicar un algoritmo a un número amplio de casos.


### Expresiones regulares

Las expresiones regulares son formas de sintaxis que permiten encontrar patrones en textos. Resultan tremendamente útiles a la hora de eliminar espacios en blanco, remover puntuación o acentos. Permite, además, encontrar palabras o números según patrones concretos. Su uso nos facilita buscar información, eliminar secciones que no nos sirven y evitar errores.

Por ejemplo, el código abajo remueve los dobles espacios en blanco del texto:

```{r, eval=T, class.output="vectout"}

# Crea una variable con muchos espacios
tx <- "Este    texto      tiene    muchos espacios en      blanco."

# Sustituye los múltiples espacios por solo uno 
gsub("\\s+"," ", tx)

# Sustituye dos espacios por uno 
gsub("\\s{2}"," ", tx)

```

La función *gsub()* sirve para reemplazar textos en una variable. En el primer ejemplo, la expresión regular *\\\\s+* indica al R que busque cualquier secuencia de texto en la que haya un espacio en blanco o más y la reemplaza por solo un espacio. En la segunda, *\\\\s{2}* busca dos espacios y los sustituye por uno. Como vemos, los resultados son distintos porque hemos solicitado que R hiciera búsquedas diferentes.

Imaginemos que hay una variable de texto y necesitamos encontrar todos los números contenidos en ella. La función *str_extract_all()* del paquete *stringi* permite extraer información de una variable de texto. Si la combinamos con la expresión regular *\\\\d+* (dígitos numéricos), el resultado es un conjunto de números.

```{r, eval=T, class.output="vectout"}

# Crea una variable textos conteniendo números
tx <- c("Tengo 10 euros y debo 1000.",
        "De los 18 equipos, sono 1 puede llegar a campeón.", 
        "Más vale 8 que 80.")

# Carga el paquete
library(stringi)

# Extrae los números 
stri_extract_all(tx, regex = "\\d+")

```

En el ejemplo abajo, se utiliza otra expresión regular *[A-Z]* (mayúsculas), luego *[a-z]** (seguida de minúsculas) para encontrar y extraer las palabras iniciadas en mayúsculas en el texto.

```{r, eval=T, class.output="vectout"}

# Carga el paquete
library(stringi)

# Crea un texto de ejemplo
tx <- "Aqui pondremos algunos Ministerios, la Presidencia y el presidente."

## Extrae del texto expresiones con mayusculas
stri_extract_all(tx,regex = "[A-Z][a-z]*")

```

También se puede dividir un texto utilizando un caracter o palabra. La función *stri_split()* fragmenta una variable de texto a partir de un patrón que puede ser un caracter, como un espacio, una palabra, o un símbolo, como el de salto de linea. 

```{r, eval=T, class.output="vectout"}

# Carga el paquete
library(stringi)

## Define el texto a ser dividido
tx <- c("Esta es la primera frase.\nEsta es la segunda frase.")

## Divide utilizando salto de linea
stri_split(tx, regex = "\n")

## Divide utilizando espacio
stri_split(tx, regex = " ")

```

¿Ya has intentado comparar los términos con o sin tildes? Acentuación y puntuación representan obstáculos comunes para la comparación de textos, especialmente cuando se aplican técnicas como la de *bag-of-words*. En estos casos, *aquí*, *aqui* y *aquí.* son consideradas palabras distintas. Para ello, hace falta remover la puntuación y los acentos para poder compararlas y encontrar su semejanza.  

```{r, eval=T, class.output="vectout"}

# Carga el paquete
library(stringi)

## Declara el texto
tx <- c("José, María y Elena quieren ir a la fiesta de ensueño. Pero, ¿de qué fiesta hablas, Pepe?")

# Elimina la puntuacion
stri_replace_all(tx, regex = "[:punct:]","")

# Elimina todos los acentos
stri_trans_general(tx, "Latin-ASCII")

```

Estos ejemplos constituyen una pequeña introducción a las expresiones regulares. Hay un mundo de referencias a ser exploradas y hace falta tener siempre a mano un conjunto de [chuletas](https://quickref.me/regex) para ayudarnos a buscar patrones de texto dependiendo del tipo de texto que estamos utilizando en cada momento. 

**Referencias adicionales**

Existe un enorme material disponible sobre expresiones regulares, os recomiendo los siguientes:

1. <a href="https://r4ds.had.co.nz/strings.html" target="_blank">Wickam - "Strings" En *R for Data Science*</a>

2. <a href="https://cran.r-project.org/web/packages/stringr/vignettes/regular-expressions.html" target="_blank">"Regular Expressions" en la documentación del paquete *stringr*</a>

3. <a href="https://ladal.edu.au/regex.html" target="_blank">"Regular Expressions" en el laboratorio *LADAU*</a>

4. <a href="https://regex101.com/" target="_blank">*Regular Expressions 101* es una página obligatoria para aquellos que quieran emplear expresiones regulares.</a>

También vale la pena consultar las referencias de los dos paquetes más importantes para la manipulación de datos en R, el *stringr* y el *stringi*:

1. <a href="https://stringi.gagolewski.com/" target="_blank">stringi: Fast and Portable Character String Processing in R</a>

2. <a href="https://stringr.tidyverse.org/index.html" target="_blank">stringr</a>




